"""Behavioural Cloning (BC). Trains policy by applying supervised learning to a
fixed dataset of (observation, action) pairs generated by some expert
demonstrator."""

import os
from typing import Type

import cloudpickle
from stable_baselines.common.dataset import Dataset
from stable_baselines.common.policies import ActorCriticPolicy
import tensorflow as tf
from tqdm.autonotebook import tqdm, trange

from imitation.policies.base import FeedForward32Policy
from imitation.util import rollout


class BCTrainer:
  """Simple behavioural cloning (BC).

  Recovers only a policy.

  Args:
    env (gym.Env): environment to train on.
    expert_rollouts: A tuple of four arrays from expert rollouts,
        `obs`, `act`, `next_obs`, `reward`.
    policy_class (BasePolicy): used to instantiate imitation policy.
    batch_size (int): batch size used for training.
    """

  def __init__(self,
               env,
               *,
               expert_demos: rollout.Transitions,
               policy_class: Type[ActorCriticPolicy] = FeedForward32Policy,
               batch_size: int = 32):
    self.env = env
    self.policy_class = policy_class
    self.batch_size = batch_size
    self.expert_dataset = Dataset(
        {
            "obs": expert_demos.obs,
            "act": expert_demos.acts,
        }, shuffle=True)
    self.graph = tf.Graph()
    self.sess = tf.Session(graph=self.graph)
    with self.graph.as_default():
      self._build_tf_graph()
      self.sess.run(tf.global_variables_initializer())

  def train(self, *, n_epochs=100):
    """Train with supervised learning for some number of epochs.

    Here an 'epoch' is just a complete pass through the expert transition
    dataset.

    Args:
      n_epochs (int): number of complete passes made through dataset.
    """
    epoch_iter = trange(n_epochs, desc='BC epoch')
    for epoch_num in epoch_iter:
      total_batches = self.expert_dataset.n_samples // self.batch_size
      batch_iter = self.expert_dataset.iterate_once(self.batch_size)
      tq_iter = tqdm(
          batch_iter, total=total_batches, desc='pol step', leave=False)
      loss_ewma = None
      for batch_dict in tq_iter:
        feed_dict = {
            self._true_acts_ph: batch_dict['act'],
            self.policy.obs_ph: batch_dict['obs'],
        }
        _, loss = self.sess.run(
            [self._train_op, self._log_loss], feed_dict=feed_dict)
        tq_iter.set_postfix(loss='% 3.4f' % loss)
        if loss_ewma is None:
          loss_ewma = loss
        else:
          loss_ewma = 0.9 * loss_ewma + 0.1 * loss
      epoch_iter.set_postfix(loss_ewma='% 3.4f' % loss_ewma)

  def test_policy(self, *, min_episodes=10):
    """Test current imitation policy on environment & give some rollout
    stats.

    Args:
      min_episodes (int): Minimum number of rolled-out episodes.

    Returns:
      dict: rollout statistics collected by
        `imitation.utils.rollout.rollout_stats()`.
    """
    trajs = rollout.generate_trajectories(
        self.policy, self.env, sample_until=rollout.min_episodes(min_episodes))
    reward_stats = rollout.rollout_stats(trajs)
    return reward_stats

  def _build_tf_graph(self):
    with tf.variable_scope('bc_supervised_loss'):
      with tf.variable_scope('model'):
        self.policy_kwargs = dict(
            ob_space=self.env.observation_space,
            ac_space=self.env.action_space,
            n_batch=None,
            n_env=1,
            n_steps=1000)
        self.policy = self.policy_class(
          sess=self.sess,
          **self.policy_kwargs)  # pytype: disable=not-instantiable
        inner_scope = tf.get_variable_scope().name
        self.policy_variables = tf.get_collection(
          tf.GraphKeys.TRAINABLE_VARIABLES, scope=inner_scope)
      self._true_acts_ph = self.policy.pdtype.sample_placeholder(
          [None], name='ref_acts_ph')
      self._log_loss = tf.reduce_mean(
          self.policy.proba_distribution.neglogp(self._true_acts_ph))
      # FIXME: it should be possible to customise both optimiser class and
      # optimiser arguments
      opt = tf.train.AdamOptimizer()
      self._train_op = opt.minimize(self._log_loss)

  def save_policy(self, policy_path):
    """Save a policy to a pickle. Can be reloaded by `.reconstruct_policy()`.

    Args:
        policy_path (str): path to save policy to."""
    policy_params = self.sess.run(self.policy_variables)
    data = {
      'class': self.policy_class,
      'kwargs': self.policy_kwargs,
      'params': policy_params,
    }
    dirname = os.path.dirname(policy_path)
    if dirname:
      os.makedirs(dirname, exist_ok=True)
    with open(policy_path, 'wb') as fp:
      cloudpickle.dump(data, fp)

  @staticmethod
  def reconstruct_policy(policy_path, sess=None):
    """Reconstruct a saved policy.

    Args:
        policy_path (str): path a policy produced by `.save_policy()`.
        sess (tf.Session or None): optional session to construct policy under,
          if not the default session.

    Returns:
        policy (BasePolicy): policy with reloaded weights."""
    if sess is None:
      sess = tf.get_default_session()
      assert sess is not None, "must supply session via kwarg or context mgr"

    # re-read data from dict
    with open(policy_path, 'rb') as fp:
      loaded_pickle = cloudpickle.load(fp)

    # construct the policy class
    klass = loaded_pickle['class']
    kwargs = loaded_pickle['kwargs']
    rv_pol = klass(sess=sess, **kwargs)

    # set values for the new policy's parameters
    param_values = loaded_pickle['params']
    inner_scope = tf.get_variable_scope().name
    tf_vars = tf.get_collection(
      tf.GraphKeys.TRAINABLE_VARIABLES, scope=inner_scope)
    placeholders = [
      tf.placeholder(shape=v.shape, dtype=v.dtype) for v in tf_vars
    ]
    assign_ops = [tf.assign(var, ph) for var, ph in zip(tf_vars, placeholders)]
    sess.run(assign_ops, feed_dict={
      ph: value for ph, value in zip(placeholders, param_values)
    })

    return rv_pol
