import collections
import functools
import os
import pickle
from tqdm import tqdm
from typing import Callable, Dict, List, NamedTuple, Optional, Sequence, Union

import numpy as np
from stable_baselines.common.base_class import BaseRLModel
from stable_baselines.common.policies import BasePolicy
from stable_baselines.common.vec_env import VecEnv
import tensorflow as tf

from imitation.policies.base import get_action_policy
from imitation.util.reward_wrapper import RewardFn


class Trajectory(NamedTuple):
  """A trajectory, e.g. a one episode rollout from an expert policy.

   Attributes:
    obs: Observations, shape (trajectory_len+1, ) + observation_shape.
    act: Actions, shape (trajectory_len, ) + action_shape.
    rew: Reward, shape (trajectory_len, ).
    infos: A list of info dicts, length (trajectory_len, ).
  """

  acts: np.ndarray
  obs: np.ndarray
  rews: np.ndarray
  infos: Optional[List[dict]]


def unwrap_traj(traj: Trajectory) -> Trajectory:
  """Uses `MonitorPlus`-captured `obs` and `rews` to replace fields.

  This can be useful for bypassing other wrappers to retrieve the original
  `obs` and `rews`.

  Fails if `infos` is None or if the Trajectory was generated from an
  environment without imitation.util.MonitorPlus.

  Args:
    traj: A Trajectory generated from `MonitorPlus`-wrapped Environments.

  Returns:
    A copy of `traj` with replaced `obs` and `rews` fields.
  """
  ep_info = traj.infos[-1]["episode"]
  res = traj._replace(obs=ep_info["obs"], rews=ep_info["rews"])
  assert len(res.obs) == len(res.acts) + 1
  assert len(res.rews) == len(res.acts)
  return res


def recalc_rewards_traj(traj: Trajectory, reward_fn: RewardFn) -> np.ndarray:
  """Returns the rewards of the trajectory calculated under a diff reward fn."""
  steps = np.arange(len(traj.rews))
  return reward_fn(traj.obs[:-1], traj.acts, traj.obs[1:], steps)


class Transitions(NamedTuple):
  """A batch of obs-act-obs-rew-done transitions.

  Usually generated by combining and processing several Trajectories via
  `flatten_trajectory()`.

  Attributes:
    obs: Previous observations. Shape: (batch_size, ) + observation_shape.
        The i'th observation `obs[i]` in this array is the observation seen
        by the agent when choosing action `act[i]`.
    act: Actions. Shape: (batch_size, ) + action_shape.
    next_obs: New observation. Shape: (batch_size, ) + observation_shape.
        The i'th observation `next_obs[i]` in this array is the observation
        after the agent has taken action `act[i]`.
    rew: Reward. Shape: (batch_size, ).
        The reward `rew[i]` at the i'th timestep is received after the agent has
        taken action `act[i]`.
    done: Boolean array indicating episode termination. Shape: (batch_size, ).
        `done[i]` is true iff `next_obs[i]` the last observation of an episode.
  """

  obs: np.ndarray
  acts: np.ndarray
  next_obs: np.ndarray
  rews: np.ndarray
  dones: np.ndarray


class _TrajectoryAccumulator:
  """Accumulates trajectories step-by-step.

  Used in `generate_trajectories()` only, for collecting completed trajectories
  while ignoring partially-completed trajectories.
  """

  def __init__(self):
    self.partial_trajectories = collections.defaultdict(list)

  def finish_trajectory(self, idx) -> Trajectory:
    """Complete the trajectory labelled with `idx`.

    Return list of completed trajectories popped from
    `self.partial_trajectories`.
    """
    part_dicts = self.partial_trajectories[idx]
    del self.partial_trajectories[idx]
    out_dict_unstacked = collections.defaultdict(list)
    for part_dict in part_dicts:
      for key, array in part_dict.items():
        out_dict_unstacked[key].append(array)
    out_dict_stacked = {
        key: np.stack(arr_list, axis=0)
        for key, arr_list in out_dict_unstacked.items()
    }
    return Trajectory(**out_dict_stacked)

  def add_step(self, idx, step_dict: Dict[str, np.ndarray]):
    """Add a single step to the partial trajectory identified by `idx`.

    This could correspond to, e.g., one environment managed by a VecEnv.
    """
    self.partial_trajectories[idx].append(step_dict)


GenTrajTerminationFn = Callable[[Sequence[Trajectory], _TrajectoryAccumulator], bool]

class _GenTrajUntilTimesteps(object):
  def __init__(self, n: int):
    self._n = n
    self._pbar = tqdm(total=n, desc='Rollouts', ncols=80)
  def __call__(self, trajs: Sequence[Trajectory], trajs_accum: _TrajectoryAccumulator):
    # timesteps of finished trajectories
    timesteps = sum(len(t.obs) - 1 for t in trajs)
    # timesteps of trajectories in progress
    timesteps_accum = sum(len(t) - 1 for t in trajs_accum.partial_trajectories.values())
    # update progress bar
    inc = max(min(self._n, timesteps + timesteps_accum) - self._pbar.n, 0)
    if inc > 0:
      self._pbar.update(inc)
    # note we are not interested in partial trajectories for now
    if timesteps >= self._n:
      self._pbar.close()
      return True
    return False

class _GenTrajUntilEpisodes(object):
  def __init__(self, n: int):
    self._n = n
    self._pbar = tqdm(total=n, desc='Rollouts', ncols=80, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}]')
  def __call__(self, trajs: Sequence[Trajectory], trajs_accum: _TrajectoryAccumulator):
    episodes = len(trajs)
    # update progress bar
    inc = max(min(self._n, episodes) - self._pbar.n, 0)
    if inc > 0:
      self._pbar.update(inc)
    # note we are not interested in partial trajectories for now
    if episodes >= self._n:
      self._pbar.close()
      return True
    return False

min_timesteps = _GenTrajUntilTimesteps
min_episodes = _GenTrajUntilEpisodes

def make_sample_until(n_timesteps: Optional[int],
                      n_episodes: Optional[int],
                      ) -> GenTrajTerminationFn:
  """Returns a termination condition sampling until n_timesteps or n_episodes.

  Arguments:
    n_timesteps: Minimum number of timesteps to sample.
    n_episodes: Number of episodes to sample.

  Returns:
    A termination condition.

  Raises:
    ValueError if both or neither of n_timesteps and n_episodes are set,
    or if either are non-positive.
  """
  if n_timesteps is not None and n_episodes is not None:
    raise ValueError("n_timesteps and n_episodes were both set")
  elif n_timesteps is not None:
    assert n_timesteps > 0
    return min_timesteps(n_timesteps)
  elif n_episodes is not None:
    assert n_episodes > 0
    return min_episodes(n_episodes)
  else:
    raise ValueError("Set at least one of n_timesteps and n_episodes")


def generate_trajectories(policy,
                          venv: VecEnv,
                          sample_until: GenTrajTerminationFn,
                          *,
                          deterministic_policy: bool = False,
                          ) -> Sequence[Trajectory]:
  """Generate trajectory dictionaries from a policy and an environment.

  Args:
    policy (BasePolicy or BaseRLModel): A stable_baselines policy or RLModel,
        trained on the gym environment.
    venv: The vectorized environments to interact with.
    sample_until: A function determining the termination condition.
        It takes a sequence of trajectories, and returns a bool.
        Most users will want to use one of `min_episodes` or `min_timesteps`.
    deterministic_policy: If True, asks policy to deterministically return
        action. Note the trajectories might still be non-deterministic if the
        environment has non-determinism!

  Returns:
    Sequence of `Trajectory` named tuples.
  """
  if isinstance(policy, BaseRLModel):
    get_action = policy.predict
    policy.set_env(venv)
  else:
    get_action = functools.partial(get_action_policy, policy)

  # Collect rollout tuples.
  trajectories = []
  # accumulator for incomplete trajectories
  trajectories_accum = _TrajectoryAccumulator()
  obs_batch = venv.reset()
  for env_idx, obs in enumerate(obs_batch):
    # Seed with first obs only. Inside loop, we'll only add second obs from
    # each (s,a,r,s') tuple, under the same "obs" key again. That way we still
    # get all observations, but they're not duplicated into "next obs" and
    # "previous obs" (this matters for, e.g., Atari, where observations are
    # really big).
    trajectories_accum.add_step(env_idx, dict(obs=obs))
  while not sample_until(trajectories, trajectories_accum):
    obs_old_batch = obs_batch
    act_batch, _ = get_action(obs_old_batch, deterministic=deterministic_policy)
    obs_batch, rew_batch, done_batch, info_batch = venv.step(act_batch)
    # Don't save tuples if there is a done. The next_obs for any environment
    # is incorrect for any timestep where there is an episode end, so we fix it
    # with returned state info.
    zip_iter = enumerate(
        zip(obs_old_batch, act_batch, obs_batch, rew_batch, done_batch,
            info_batch))
    for env_idx, (obs_old, act, obs, rew, done, info) in zip_iter:
      real_obs = obs
      if done:
        # actual obs is inaccurate, so we use the one inserted into step info
        # by stable baselines wrapper
        real_obs = info['terminal_observation']
      trajectories_accum.add_step(
          env_idx,
          dict(
              acts=act,
              rews=rew,
              # this is not the obs corresponding to `act`, but rather the obs
              # *after* `act` (see above)
              obs=real_obs,
              infos=info))
      if done:
        # finish env_idx-th trajectory
        new_traj = trajectories_accum.finish_trajectory(env_idx)
        trajectories.append(new_traj)
        trajectories_accum.add_step(env_idx, dict(obs=obs))
        continue

  # Note that we just drop partial trajectories. This is not ideal for some
  # algos; e.g. BC can probably benefit from partial trajectories, too.

  # Sanity checks.
  for trajectory in trajectories:
    n_steps = len(trajectory.acts)
    # extra 1 for the end
    exp_obs = (n_steps + 1, ) + venv.observation_space.shape
    real_obs = trajectory.obs.shape
    assert real_obs == exp_obs, f"expected shape {exp_obs}, got {real_obs}"
    exp_act = (n_steps, ) + venv.action_space.shape
    real_act = trajectory.acts.shape
    assert real_act == exp_act, f"expected shape {exp_act}, got {real_act}"
    exp_rew = (n_steps,)
    real_rew = trajectory.rews.shape
    assert real_rew == exp_rew, f"expected shape {exp_rew}, got {real_rew}"

  return trajectories


def rollout_stats(trajectories: Sequence[Trajectory]) -> dict:
  """Calculates various stats for a sequence of trajectories.

  Args:
      trajectories: Sequence of `Trajectory`.

  Returns:
      Dictionary containing `n_traj` collected (int), along with episode return
      statistics (keys: `{monitor_,}return_{min,mean,std,max}`, float values)
      and trajectory length statistics (keys: `len_{min,mean,std,max}`, float
      values).

      `return_*` values are calculated from environment rewards.
      `monitor_*` values are calculated from Monitor-captured rewards, and
      are only included if the `trajectories` contain Monitor infos.
  """
  assert len(trajectories) > 0
  out_stats = {"n_traj": len(trajectories)}
  traj_descriptors = {
    "return": np.asarray([sum(t.rews) for t in trajectories]),
    "len": np.asarray([len(t.rews) for t in trajectories]),
  }

  infos_peek = trajectories[0].infos
  if infos_peek is not None and "episode" in infos_peek[-1]:
    monitor_ep_returns = [t.infos[-1]["episode"]["r"] for t in trajectories]
    traj_descriptors["monitor_return"] = np.asarray(monitor_ep_returns)

  stat_names = ["min", "mean", "std", "max"]
  for desc_name, desc_vals in traj_descriptors.items():
    for stat_name in stat_names:
      stat_value = getattr(np, stat_name)(desc_vals)
      out_stats[f"{desc_name}_{stat_name}"] = stat_value
  return out_stats


def mean_return(*args, **kwargs) -> float:
  """Find the mean return of a policy.

  Shortcut to call `generate_trajectories` and fetch the `rollout_stats` value
  for `'return_mean'`; see documentation for `generate_trajectories` and
  `rollout_stats`.
  """
  trajectories = generate_trajectories(*args, **kwargs)
  return rollout_stats(trajectories)["return_mean"]


def flatten_trajectories(trajectories: Sequence[Trajectory]) -> Transitions:
  """Flatten a series of trajectory dictionaries into arrays.

  Returns observations, actions, next observations, rewards.

  Args:
      trajectories: list of trajectories.

  Returns:
    The trajectories flattened into a single batch of Transitions.
  """
  keys = ["obs", "next_obs", "acts", "rews", "dones"]
  parts = {key: [] for key in keys}
  for traj in trajectories:
    parts["acts"].append(traj.acts)
    parts["rews"].append(traj.rews)
    obs = traj.obs
    parts["obs"].append(obs[:-1])
    parts["next_obs"].append(obs[1:])
    dones = np.zeros_like(traj.rews, dtype=np.bool)
    dones[-1] = True
    parts["dones"].append(dones)
  cat_parts = {
    key: np.concatenate(part_list, axis=0)
    for key, part_list in parts.items()
  }
  lengths = set(map(len, cat_parts.values()))
  assert len(lengths) == 1, f"expected one length, got {lengths}"
  return Transitions(**cat_parts)


def generate_transitions(policy,
                         venv,
                         n_timesteps: int,
                         *,
                         truncate: bool = True,
                         **kwargs) -> Transitions:
  """Generate obs-action-next_obs-reward tuples.

  Args:
    policy (BasePolicy or BaseRLModel): A stable_baselines policy or RLModel,
        trained on the gym environment.
    venv: The vectorized environments to interact with.
    n_timesteps: The minimum number of timesteps to sample.
    truncate: If True, then drop any additional samples to ensure that exactly
        `n_timesteps` samples are returned.
    **kwargs: Passed-through to generate_trajectories.

  Returns:
    A batch of Transitions. The length of the constituent arrays is guaranteed
    to be at least `n_timesteps` (if specified), but may be greater unless
    `truncate` is provided as we collect data until the end of each episode.
  """
  traj = generate_trajectories(policy, venv,
                               sample_until=min_timesteps(n_timesteps),
                               **kwargs)
  transitions = flatten_trajectories(traj)
  if truncate and n_timesteps is not None:
    transitions = Transitions(*(arr[:n_timesteps] for arr in transitions))
  return transitions


def save(path: str,
         policy: Union[BaseRLModel, BasePolicy],
         venv: VecEnv,
         sample_until: GenTrajTerminationFn,
         *,
         unwrap: bool = True,
         exclude_infos: bool = True,
         **kwargs,
         ) -> None:
    """Generate policy rollouts and save them to a pickled Sequence[Trajectory].

    The `.infos` field of each Trajectory is set to `None` to save space.

    Args:
      path: Rollouts are saved to this path.
      venv: The vectorized environments.
      sample_until: End condition for rollout sampling.
      unwrap: If True, then save original observations and rewards (instead of
        potentially wrapped observations and rewards) by calling
        `unwrap_traj()`.
      exclude_infos: If True, then exclude `infos` from pickle by setting
        this field to None. Excluding `infos` can save a lot of space during
        pickles.
      deterministic_policy: Argument from `generate_trajectories`.
    """
    os.makedirs(os.path.dirname(path), exist_ok=True)
    trajs = generate_trajectories(policy, venv, sample_until, **kwargs)
    if unwrap:
      trajs = [unwrap_traj(traj) for traj in trajs]
    if exclude_infos:
      trajs = [traj._replace(infos=None) for traj in trajs]

    with open(path, "wb") as f:
      pickle.dump(trajs, f)
    tf.logging.info("Dumped demonstrations to {}.".format(path))
